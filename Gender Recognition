{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8899631,"sourceType":"datasetVersion","datasetId":5328253}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **DISCLAIMER: I used AI to automatically generate the markdown comments (Do not really believe them). The code is mine.**","metadata":{}},{"cell_type":"markdown","source":"# Image Classification with PyTorch: Gender Recognition\n\n## 1. Introduction\n\nWelcome to this Kaggle notebook on image classification using PyTorch! In this tutorial, we'll be tackling the task of gender classification using facial images. Our objective is to build, train, and evaluate a Convolutional Neural Network (CNN) that can accurately predict whether an image contains a male or female face.\n\nThis notebook is designed to guide you through the entire process of creating an image classification model, from data preparation to model evaluation. We'll cover important concepts in deep learning and computer vision, making this an excellent resource for students and practitioners alike.\n\nLet's get started!","metadata":{}},{"cell_type":"markdown","source":"## 2. Setup\n\nFirst, we need to import the necessary libraries and set up our environment. Each library we're using serves a specific purpose in our machine learning pipeline.","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting the random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Setting the paths\nTRAIN_PATH = Path(\"/kaggle/input/gender-recognizer/dataset\")\nIMAGE_PATH_LIST = list(TRAIN_PATH.glob(\"*/*.jpg\"))\n\nprint(f\"Total number of images: {len(IMAGE_PATH_LIST)}\")\nprint(f\"Sample image path: {IMAGE_PATH_LIST[0]}\")\nprint(f\"Sample image path 2: {IMAGE_PATH_LIST[-1]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's break down the purpose of each imported library:\n\n- `os`, `pathlib`: For handling file and directory paths\n- `numpy`, `pandas`: For numerical operations and data manipulation\n- `PIL`: For opening and processing images\n- `sklearn`: For data splitting and evaluation metrics\n- `torch`, `torch.nn`, `torch.optim`: Core PyTorch libraries for building and training neural networks\n- `torchvision.transforms`: For image transformations and augmentations\n- `tqdm`: For progress bars during training\n- `matplotlib`, `seaborn`: For creating visualizations","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preparation\n\n### 3.1 Custom Dataset Class\n\nWe'll create a custom `GenderDataset` class that inherits from `torch.utils.data.Dataset`. This class will handle loading and preprocessing our images.","metadata":{}},{"cell_type":"code","source":"class GenderDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n        self.labels = [0 if 'woman' in str(path).lower() else 1 for path in image_paths]\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Let's check the distribution of our dataset\nlabel_counts = Counter(0 if 'woman' in str(path).lower() else 1 for path in IMAGE_PATH_LIST)\nprint(\"Class distribution:\")\nprint(f\"Female (0): {label_counts[0]}\")\nprint(f\"Male (1): {label_counts[1]}\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Female', 'Male'], [label_counts[0], label_counts[1]])\nplt.title('Class Distribution')\nplt.ylabel('Number of Images')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `GenderDataset` class does the following:\n1. Stores the list of image paths and the transformation pipeline\n2. Creates labels based on the image filenames (1 for male, 0 for female)\n3. Implements `__len__` to return the total number of images\n4. Implements `__getitem__` to load and preprocess an image when indexed","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Data Augmentation and Normalization\n\nData augmentation helps our model generalize better by exposing it to various transformations of the input data. Normalization ensures that our input features are on a similar scale, which can help with model convergence.","metadata":{}},{"cell_type":"code","source":"# Data augmentation and normalization\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Splitting the data into train and validation sets\ntrain_paths, val_paths = train_test_split(IMAGE_PATH_LIST, test_size=0.2, random_state=42)\ntrain_dataset = GenderDataset(train_paths, transform=transform)\nval_dataset = GenderDataset(val_paths, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's break down our transformation pipeline:\n1. `Resize`: Ensures all images are the same size (128x128 pixels)\n2. `RandomHorizontalFlip`: Randomly flips images horizontally, increasing dataset variety\n3. `ToTensor`: Converts images to PyTorch tensors\n4. `Normalize`: Normalizes the image using mean and std dev values typical for ImageNet","metadata":{}},{"cell_type":"markdown","source":"### 3.3 Visualizing Sample Images\n\nLet's visualize some sample images from our dataset to get a better understanding of what we're working with.","metadata":{}},{"cell_type":"code","source":"def imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n\n# Get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show images\nfig = plt.figure(figsize=(15, 15))\nfor i in range(9):\n    ax = fig.add_subplot(3, 3, i+1, xticks=[], yticks=[])\n    imshow(images[i])\n    ax.set_title(f\"{'Male' if labels[i] == 1 else 'Female'}\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This visualization helps us understand the nature of our input data and confirm that our data loading process is working correctly.","metadata":{}},{"cell_type":"markdown","source":"## 4. Model Definition\n\nNow, let's define our Convolutional Neural Network (CNN) model.","metadata":{}},{"cell_type":"code","source":"class CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n        self.fc2 = nn.Linear(512, 2)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 16 * 16)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nmodel = CustomCNN()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our `CustomCNN` class defines a simple convolutional neural network with the following architecture:\n1. Three convolutional layers (Conv2d) with ReLU activation and max pooling\n2. Two fully connected layers\n3. Dropout for regularization\n\nThe `forward` method defines how data flows through the network.","metadata":{}},{"cell_type":"markdown","source":"## 5. Training the Model\n\nNow that we have our data and model ready, let's set up the training process.","metadata":{}},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    train_losses = []\n    train_accuracies = []\n    val_losses = []\n    val_accuracies = []\n    \n    # Initial evaluation\n    val_loss, val_acc = evaluate_model(model, val_loader)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc.item())\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            train_correct += torch.sum(preds == labels.data)\n        \n        train_loss = train_loss / len(train_loader.dataset)\n        train_acc = train_correct.double() / len(train_loader.dataset)\n        \n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc.item())\n        \n        val_loss, val_acc = evaluate_model(model, val_loader)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc.item())\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    return train_losses, train_accuracies, val_losses, val_accuracies\n\n# Evaluation function\ndef evaluate_model(model, val_loader):\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_preds = []\n    val_labels = []\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_correct += torch.sum(preds == labels.data)\n            \n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n    \n    val_loss = val_loss / len(val_loader.dataset)\n    val_acc = val_correct.double() / len(val_loader.dataset)\n    \n    return val_loss, val_acc\n\n# Train the model\ntrain_losses, train_accuracies, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n\n# Plot training and validation loss\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot training and validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Training Accuracy')\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training process involves:\n1. Iterating over the training data for a specified number of epochs\n2. For each batch, performing a forward pass, calculating the loss, and updating the model parameters\n3. Evaluating the model on the validation set after each epoch\n4. Tracking and plotting the training and validation loss and accuracy","metadata":{}},{"cell_type":"markdown","source":"## 6. Evaluation\n\nNow that we've trained our model, let's evaluate its performance in more detail.","metadata":{}},{"cell_type":"code","source":"def detailed_evaluate_model(model, val_loader):\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_preds = []\n    val_labels = []\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_correct += torch.sum(preds == labels.data)\n            \n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n    \n    val_loss = val_loss / len(val_loader.dataset)\n    val_acc = val_correct.double() / len(val_loader.dataset)\n    \n    # Classification report\n    target_names = [\"Female\", \"Male\"]\n    print(\"Classification Report:\")\n    print(classification_report(val_labels, val_preds, target_names=target_names))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(val_labels, val_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    \n    return val_loss, val_acc\n\nprint(\"Final model performance:\")\nval_loss, val_acc = detailed_evaluate_model(model, val_loader)\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This detailed evaluation provides:\n1. A classification report showing precision, recall, and F1-score for each class\n2. A confusion matrix visualizing the model's predictions","metadata":{}},{"cell_type":"markdown","source":"## 7. Model Saving and Final Evaluation\n\nLet's save our trained model and perform a final evaluation.","metadata":{}},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), 'model.pth')\nprint(\"Model saved to 'model.pth'.\")\n\n# Load the model\nmodel_loaded = CustomCNN()\nmodel_loaded.load_state_dict(torch.load('model.pth'))\nmodel_loaded.to(device)\n\n# Evaluate the loaded model\nprint(\"Evaluating loaded model...\")\nval_loss, val_acc = detailed_evaluate_model(model_loaded, val_loader)\nprint(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our final steps include saving the trained model to disk and loading it for evaluation. This ensures that our model's performance is consistent even after being saved and loaded.\n\n## Conclusion\n\nIn this notebook, we have successfully:\n1. Loaded and preprocessed the dataset\n2. Defined and trained a convolutional neural network\n3. Evaluated the model's performance\n4. Saved and reloaded the model to verify its consistency\n\nFeel free to experiment with different architectures, hyperparameters, and data augmentation techniques to further improve the model's performance.","metadata":{}}]}