{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8899631,"sourceType":"datasetVersion","datasetId":5328253},{"sourceId":79784,"sourceType":"modelInstanceVersion","modelInstanceId":67038,"modelId":92062}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/benjamindup/94-gender-recognition-resnet18?scriptVersionId=189052575\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## **Disclaimer**\n**This is a personnal notebook, comments are AI generated (but code is mine). Do not rely on them.**","metadata":{}},{"cell_type":"markdown","source":"# Image Classification with PyTorch: Gender Recognition\n\n## 1. Introduction\n\nWelcome to this Kaggle notebook on image classification using PyTorch! In this tutorial, we'll be tackling the task of gender classification using facial images. Our objective is to build, train, and evaluate a Convolutional Neural Network (CNN) that can accurately predict whether an image contains a male or female face.\n\nThis notebook is designed to guide you through the entire process of creating an image classification model, from data preparation to model evaluation. We'll cover important concepts in deep learning and computer vision, making this an excellent resource for students and practitioners alike.\n\nLet's get started!","metadata":{}},{"cell_type":"markdown","source":"## 2. Setup\n\nFirst, we need to import the necessary libraries and set up our environment. Each library we're using serves a specific purpose in our machine learning pipeline.","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torchvision.models import resnet18\n\n# Setting the random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Setting the paths\nTRAIN_PATH = Path(\"/kaggle/input/gender-recognizer/dataset\")\nIMAGE_PATH_LIST = list(TRAIN_PATH.glob(\"*/*.jpg\"))\n\nprint(f\"Total number of images: {len(IMAGE_PATH_LIST)}\")\nprint(f\"Sample image path: {IMAGE_PATH_LIST[0]}\")\nprint(f\"Sample image path 2: {IMAGE_PATH_LIST[-1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:11.851014Z","iopub.execute_input":"2024-07-20T11:00:11.851305Z","iopub.status.idle":"2024-07-20T11:00:19.175348Z","shell.execute_reply.started":"2024-07-20T11:00:11.851279Z","shell.execute_reply":"2024-07-20T11:00:19.174436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's break down the purpose of each imported library:\n\n- `os`, `pathlib`: For handling file and directory paths\n- `numpy`, `pandas`: For numerical operations and data manipulation\n- `PIL`: For opening and processing images\n- `sklearn`: For data splitting and evaluation metrics\n- `torch`, `torch.nn`, `torch.optim`: Core PyTorch libraries for building and training neural networks\n- `torchvision.transforms`, `torchvision.models`: For image transformations and pre-trained models\n- `tqdm`: For progress bars during training\n- `matplotlib`, `seaborn`: For creating visualizations","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preparation\n\n### 3.1 Custom Dataset Class\n\nWe'll create a custom `GenderDataset` class that inherits from `torch.utils.data.Dataset`. This class will handle loading and preprocessing our images.","metadata":{}},{"cell_type":"code","source":"class GenderDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n        self.labels = [0 if 'woman' in str(path).lower() else 1 for path in image_paths]\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        label = self.labels[idx]\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Let's check the distribution of our dataset\nlabel_counts = Counter(0 if 'woman' in str(path).lower() else 1 for path in IMAGE_PATH_LIST)\nprint(\"Class distribution:\")\nprint(f\"Female (0): {label_counts[0]}\")\nprint(f\"Male (1): {label_counts[1]}\")\n\nplt.figure(figsize=(8, 6))\nplt.bar(['Female', 'Male'], [label_counts[0], label_counts[1]])\nplt.title('Class Distribution')\nplt.ylabel('Number of Images')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:23.366848Z","iopub.execute_input":"2024-07-20T11:00:23.36774Z","iopub.status.idle":"2024-07-20T11:00:23.577165Z","shell.execute_reply.started":"2024-07-20T11:00:23.367705Z","shell.execute_reply":"2024-07-20T11:00:23.576314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `GenderDataset` class does the following:\n1. Stores the list of image paths and the transformation pipeline\n2. Creates labels based on the image filenames (1 for male, 0 for female)\n3. Implements `__len__` to return the total number of images\n4. Implements `__getitem__` to load and preprocess an image when indexed","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Data Augmentation and Normalization\n\nData augmentation helps our model generalize better by exposing it to various transformations of the input data. Normalization ensures that our input features are on a similar scale, which can help with model convergence.","metadata":{}},{"cell_type":"code","source":"# Data augmentation and normalization\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n # Changed to 224x224 for ResNet    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Splitting the data into train and validation sets\ntrain_paths, val_paths = train_test_split(IMAGE_PATH_LIST, test_size=0.2, random_state=42)\ntrain_dataset = GenderDataset(train_paths, transform=transform)\nval_dataset = GenderDataset(val_paths, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:27.669072Z","iopub.execute_input":"2024-07-20T11:00:27.669728Z","iopub.status.idle":"2024-07-20T11:00:27.681082Z","shell.execute_reply.started":"2024-07-20T11:00:27.669692Z","shell.execute_reply":"2024-07-20T11:00:27.680113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's break down our transformation pipeline:\n1. `Resize`: Ensures all images are the same size (224x224 pixels)\n2. `RandomHorizontalFlip`: Randomly flips images horizontally, increasing dataset variety\n3. `ToTensor`: Converts images to PyTorch tensors\n4. `Normalize`: Normalizes the image using mean and std dev values typical for ImageNet","metadata":{}},{"cell_type":"markdown","source":"## 4. Model Definition\n\n### 4.1 Using Pre-trained ResNet\n\nInstead of building a CNN from scratch, we'll use a pre-trained ResNet model from `torchvision.models` and fine-tune it for our classification task. Pre-trained models have been trained on large datasets like ImageNet and can be adapted to specific tasks with transfer learning.","metadata":{}},{"cell_type":"code","source":"# Initialize model architecture without pre-trained weights\nresnet = resnet18(weights=None)\n\n# Load the model weights from the provided file\nmodel_weights_path = '/kaggle/input/resnet-18/pytorch/resnet18/1/resnet18-f37072fd.pth'\nresnet.load_state_dict(torch.load(model_weights_path))\n\n# Freeze the convolutional base\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Modify the final layer for binary classification\nnum_ftrs = resnet.fc.in_features\nresnet.fc = nn.Linear(num_ftrs, 1)  # Binary classification (male vs. female)\n\n# Move the model to the GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet = resnet.to(device)\n\nprint(\"Ready to be used\")","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:30.209266Z","iopub.execute_input":"2024-07-20T11:00:30.210103Z","iopub.status.idle":"2024-07-20T11:00:31.188681Z","shell.execute_reply.started":"2024-07-20T11:00:30.210062Z","shell.execute_reply":"2024-07-20T11:00:31.187616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Loss Function and Optimizer\n\nWe'll use the Binary Cross-Entropy loss function and the Adam optimizer for training our model.","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\noptimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)  # Only train the final layer\nprint('Optimizer and criterion setuped.')","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:32.746463Z","iopub.execute_input":"2024-07-20T11:00:32.74686Z","iopub.status.idle":"2024-07-20T11:00:32.753053Z","shell.execute_reply.started":"2024-07-20T11:00:32.746827Z","shell.execute_reply":"2024-07-20T11:00:32.751963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Training the Model\n\n### 5.1 Training Loop\n\nWe'll implement the training loop to train our model for a specified number of epochs. During each epoch, we'll iterate over the training DataLoader, compute the loss, and update the model parameters.","metadata":{}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n    train_loss_history = []\n    val_loss_history = []\n    val_accuracy_history = []\n    best_accuracy = 0.0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * images.size(0)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        train_loss_history.append(epoch_loss)\n        \n        # Validation loss and accuracy calculation\n        model.eval()\n        val_running_loss = 0.0\n        all_labels = []\n        all_preds = []\n        \n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=\"Validation\"):\n                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item() * images.size(0)\n                \n                preds = torch.round(torch.sigmoid(outputs))  # Apply sigmoid and round to get binary predictions\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(preds.cpu().numpy())\n        \n        val_loss = val_running_loss / len(val_loader.dataset)\n        val_loss_history.append(val_loss)\n        \n        # Calculate accuracy\n        accuracy = accuracy_score(all_labels, all_preds)\n        val_accuracy_history.append(accuracy)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n        \n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            torch.save(model.state_dict(), 'best_model.pth')\n            print(\"Best model saved!\")\n    \n    return train_loss_history, val_loss_history, val_accuracy_history\n\n# Train the model\nnum_epochs = 20\ntrain_loss_history, val_loss_history, val_accuracy_history = train_model(resnet, criterion, optimizer, train_loader, val_loader, num_epochs=num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:00:34.598169Z","iopub.execute_input":"2024-07-20T11:00:34.598534Z","iopub.status.idle":"2024-07-20T11:01:56.482572Z","shell.execute_reply.started":"2024-07-20T11:00:34.598506Z","shell.execute_reply":"2024-07-20T11:01:56.48148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Plotting the Loss Curves\n\nVisualizing the loss curves helps us understand the training process and identify potential issues like overfitting.","metadata":{}},{"cell_type":"code","source":"# Plotting the training and validation loss curves\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(train_loss_history, label='Training Loss')\nplt.plot(val_loss_history, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curves')\nplt.legend()\n\n# Plotting the validation accuracy\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracy_history, label='Validation Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Validation Accuracy Curve')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:02:00.433753Z","iopub.execute_input":"2024-07-20T11:02:00.434491Z","iopub.status.idle":"2024-07-20T11:02:01.113891Z","shell.execute_reply.started":"2024-07-20T11:02:00.434455Z","shell.execute_reply":"2024-07-20T11:02:01.11295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Evaluating the Model\n\n### 6.1 Loading the Best Model\n\nWe'll load the best model (with the lowest validation loss) saved during training and evaluate it on the validation set.","metadata":{}},{"cell_type":"code","source":"# Load the best model\nbest_model = models.resnet18(weights=False)\nbest_model.fc = nn.Linear(num_ftrs, 1)\nbest_model.load_state_dict(torch.load('best_model.pth'))\nbest_model = best_model.to(device)\n\n# Evaluation function\ndef evaluate_model(model, data_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for images, labels in data_loader:\n            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n            outputs = model(images)\n            preds = torch.round(torch.sigmoid(outputs))  # Apply sigmoid and round to get binary predictions\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    \n    return np.array(all_labels), np.array(all_preds)\n\n# Evaluate the best model on the validation set\nval_labels, val_preds = evaluate_model(best_model, val_loader)\n\n# Calculate accuracy\naccuracy = accuracy_score(val_labels, val_preds)\nprint(f\"Validation Accuracy: {accuracy:.4f}\")\n\n# Print classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(val_labels, val_preds, target_names=['Female', 'Male']))\n\n# Plot confusion matrix\ncm = confusion_matrix(val_labels, val_preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Female', 'Male'], yticklabels=['Female', 'Male'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:02:07.017523Z","iopub.execute_input":"2024-07-20T11:02:07.018118Z","iopub.status.idle":"2024-07-20T11:02:10.961064Z","shell.execute_reply.started":"2024-07-20T11:02:07.018084Z","shell.execute_reply":"2024-07-20T11:02:10.960137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Conclusion\n\nIn this project, we fine-tuned a pre-trained ResNet model to classify facial images as male or female using PyTorch. We went through the entire machine learning workflow, from data preprocessing to model training and evaluation. The final model achieved a good accuracy on the validation set, demonstrating the effectiveness of transfer learning for this task.\n\nFurther improvements could be made by:\n- Experimenting with different architectures or fine-tuning more layers of the ResNet model.\n- Using a larger and more diverse dataset.\n- Applying data augmentation techniques to increase the variability of the training data.\n- Tuning hyperparameters such as learning rate, batch size, and the number of epochs.\n\nThis concludes the gender classification project using a pre-trained ResNet model with PyTorch.","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\n# Function to predict gender from an image URL\ndef predict_gender_from_url(model, url):\n    response = requests.get(url)\n    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    transformed_image = transform(image).unsqueeze(0).to(device)\n    output = model(transformed_image)\n    predicted_label = torch.round(torch.sigmoid(output)).item()\n    gender = \"Male\" if predicted_label == 1 else \"Female\"\n    probability = torch.sigmoid(output).item()\n    \n    # Display the image\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n    \n    # Print the predicted gender and probability\n    print(f\"Male Probability: {probability:.4f}\")\n    print(f\"Female Probability: {1-probability:.4f}\")\n\n          \n# Example usage\nimage_url = \"https://www.byrdie.com/thmb/vwVnoCain1FIr5KwvSZorEZLZ9g=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/facialhair-26314088e12548ba9fc8af98d8500e8a.png\"  # Replace with your image URL\npredicted_gender = predict_gender_from_url(best_model, image_url)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T11:11:19.827478Z","iopub.execute_input":"2024-07-20T11:11:19.828255Z","iopub.status.idle":"2024-07-20T11:11:22.770228Z","shell.execute_reply.started":"2024-07-20T11:11:19.828222Z","shell.execute_reply":"2024-07-20T11:11:22.769315Z"},"trusted":true},"execution_count":null,"outputs":[]}]}